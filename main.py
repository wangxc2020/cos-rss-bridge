import os
import requests
import time
import xml.etree.ElementTree as ET
from datetime import datetime
from qcloud_cos import CosConfig
from qcloud_cos import CosS3Client

# === é…ç½®: æ¯ä¸ªæºåªä¿ç•™æœ€æ–°çš„ N æ¡ ===
MAX_ITEMS_PER_SOURCE = 5

# === âœ… ä¿®å¤åçš„æºåˆ—è¡¨ ===
RSS_URLS = [
    # 1. è¡Œä¸šåŸºçŸ³
    "https://openai.com/news/rss.xml",
    "https://deepmind.google/blog/rss.xml",
    "https://huggingface.co/blog/feed.xml",
    
    # 2. ä¹‹å‰æŠ¥é”™çš„æºå·²ä¿®å¤
    # Meta Engineering (åŸè·¯å¾„404ï¼Œæ”¹ç”¨ä¸»è®¢é˜…æºï¼ŒåŒ…å«AIå†…å®¹)
    "https://engineering.fb.com/feed/", 
    
    # PyTorch (å®˜æ–¹æº)
    "https://pytorch.org/blog/feed.xml",
    
    # Stability AI (å°è¯•ä¿®å¤ XML è§£æé—®é¢˜)
    "https://stability.ai/news?format=rss",
    
    # 3. ç¨³å®šæº
    "https://www.producthunt.com/feed?category=artificial-intelligence",
    "https://mshibanami.github.io/GitHubTrendingRSS/daily/python.xml",
    
    # 4. æ›¿ä»£æº
    # TechCrunch - Anthropic æ ‡ç­¾
    "https://techcrunch.com/tag/anthropic/feed/",
    # The Verge - AI æ ‡ç­¾
    "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml",
    # å›½å†…ä¿åº•
    "https://www.qbitai.com/feed"
]

# === æ ¸å¿ƒï¼šXML ç˜¦èº«å‡½æ•° ===
def truncate_xml_content(xml_text, limit=MAX_ITEMS_PER_SOURCE):
    """
    è§£æ XMLï¼Œå¼ºåˆ¶åªä¿ç•™å‰ N ä¸ª item/entryï¼Œç„¶åé‡æ–°ç”Ÿæˆå­—ç¬¦ä¸²ã€‚
    æå¤§å¹…åº¦å‡å°‘æ–‡ä»¶ä½“ç§¯ã€‚
    """
    try:
        # æ³¨å†Œå‘½åç©ºé—´é˜²æ­¢ tag å˜æˆ ns0:item
        ET.register_namespace('', "http://www.w3.org/2005/Atom")
        
        # è¿™ç§æ–¹å¼æ˜¯ä¸ºäº†å®¹é”™ï¼Œæœ‰äº› XML å£°æ˜å¯èƒ½æœ‰é—®é¢˜
        root = ET.fromstring(xml_text)
        
        # 1. å¤„ç† RSS 2.0 (<channel> -> <item>)
        channel = root.find('channel')
        if channel is not None:
            items = channel.findall('item')
            # å¦‚æœæ•°é‡è¶…è¿‡é™åˆ¶ï¼Œç§»é™¤å¤šä½™çš„
            if len(items) > limit:
                for item in items[limit:]:
                    channel.remove(item)
        
        # 2. å¤„ç† Atom (<feed> -> <entry>)
        else:
            # Atom æ ¹èŠ‚ç‚¹é€šå¸¸å°±æ˜¯ feed
            entries = root.findall('{http://www.w3.org/2005/Atom}entry')
            if not entries:
                 entries = root.findall('entry') # å°è¯•æ—  namespace
            
            if len(entries) > limit:
                for entry in entries[limit:]:
                    root.remove(entry)
                    
        # é‡æ–°è½¬å›å­—ç¬¦ä¸²
        return ET.tostring(root, encoding='unicode')
        
    except Exception as e:
        # å¦‚æœè§£æå¤±è´¥ï¼ˆå¤ªä¹±çš„æ ¼å¼ï¼‰ï¼Œä¸ºäº†å…œåº•ï¼Œè¿˜æ˜¯è¿”å›åŸæ–‡ï¼Œä½†åšå­—ç¬¦ä¸²å¼ºè¡Œæˆªæ–­
        # é¿å…å‡  MB çš„æ–‡ä»¶ä¼ ä¸Šå»
        return xml_text[:10000] 

# === å…ƒæ•°æ®åˆ†æå·¥å…· (ç”¨äºæŠ¥å‘Š) ===
def analyze_xml_simple(xml_text):
    try:
        root = ET.fromstring(xml_text)
        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry') or root.findall('.//entry')
        
        count = len(items)
        latest_date = "N/A"
        if items:
            item = items[0]
            for tag in ['pubDate', 'published', 'updated', 'dc:date']:
                node = item.find(tag)
                if node is None: node = item.find(f"{{http://www.w3.org/2005/Atom}}{tag}")
                if node is not None and node.text:
                    latest_date = node.text[:25]
                    break
        return count, latest_date
    except:
        return 0, "Parse Err"

def fetch_and_report():
    combined_data = ""
    report_lines = []
    
    print(f"{'RSS æº (Short URL)':<40} | {'Status':<6} | {'Raw Num':<7} | {'Action'}")
    print("-" * 90)
    
    report_lines.append(f"æ›´æ–°æ—¶é—´: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC")
    report_lines.append(f"ç­–ç•¥: æ¯ä¸ªæºä»…ä¿ç•™æœ€æ–°çš„ {MAX_ITEMS_PER_SOURCE} æ¡")
    report_lines.append("-" * 60)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    }

    for url in RSS_URLS:
        status_icon = "ğŸ”´"
        raw_count = 0
        action_msg = "Fail"
        short_url = url.replace("https://", "").replace("www.", "")[:38]

        try:
            resp = requests.get(url, headers=headers, timeout=25)
            
            if resp.status_code == 200 and len(resp.text) > 100:
                status_icon = "âœ…"
                
                # 1. ç˜¦èº«å¤„ç†
                lean_xml = truncate_xml_content(resp.text, limit=MAX_ITEMS_PER_SOURCE)
                
                # 2. ç»Ÿè®¡åŸå§‹æ•°é‡ vs ç˜¦èº«æ•°é‡
                raw_count, _ = analyze_xml_simple(resp.text)
                final_count, latest_date = analyze_xml_simple(lean_xml)
                
                action_msg = f"Cut {raw_count}->{final_count}"
                
                # 3. åªæœ‰çœŸæ­£æœ‰å†…å®¹æ‰åŠ å…¥ Combined
                if final_count > 0:
                    combined_data += f"\n\n<<<<SOURCE_START:{url}>>>>\n"
                    combined_data += lean_xml
                    combined_data += f"\n<<<<SOURCE_END>>>>\n"
            else:
                status_icon = "âŒ"
                action_msg = f"HTTP {resp.status_code}"

        except Exception as e:
            status_icon = "âŒ"
            action_msg = "Err"

        print(f"{short_url:<40} | {status_icon} | {raw_count:<7} | {action_msg}")
        report_lines.append(f"{status_icon} {short_url}")
        report_lines.append(f"   Items: {raw_count} -> {MAX_ITEMS_PER_SOURCE} | Last: {action_msg}")
        
        time.sleep(1)

    return combined_data, "\n".join(report_lines)

def upload_to_cos(filename, content):
    if not content: return
    secret_id = os.environ['TENCENT_SECRET_ID']
    secret_key = os.environ['TENCENT_SECRET_KEY']
    region = os.environ['COS_REGION']
    bucket = os.environ['COS_BUCKET']
    
    config = CosConfig(Region=region, SecretId=secret_id, SecretKey=secret_key)
    client = CosS3Client(config)
    
    try:
        client.put_object(
            Bucket=bucket, Body=content.encode('utf-8'), Key=filename,
            StorageClass='STANDARD', ContentType='text/plain; charset=utf-8'
        )
        print(f"ğŸ‰ Upload Success: {filename}")
    except Exception as e:
        print(f"âŒ Upload Failed {filename}: {e}")

if __name__ == "__main__":
    full_data, report_text = fetch_and_report()
    if len(full_data) > 200:
        upload_to_cos('RSS/rss_mirror.txt', full_data)
        upload_to_cos('RSS/rss_report.txt', report_text)
    else:
        print("âš ï¸ æ•°æ®é‡è¿‡å°‘ï¼Œè·³è¿‡ä¸Šä¼ ã€‚")
