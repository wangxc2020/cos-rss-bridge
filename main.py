import os
import requests
import time
import json
import re
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta
import email.utils
from qcloud_cos import CosConfig
from qcloud_cos import CosS3Client

# === âš™ï¸ é…ç½®åŒºåŸŸ ===
MAX_ITEMS = 5  # æ¯ä¸ªæºæœ€å¤šæŠ“å–æœ€æ–°çš„ N æ¡

# === ğŸ”— ä¼˜è´¨æºåˆ—è¡¨ ===
RSS_URLS = [
    # --- æ ¸å¿ƒå·¨å¤´ ---
    {"name": "OpenAI", "url": "https://openai.com/news/rss.xml"},
    {"name": "DeepMind", "url": "https://deepmind.google/blog/rss.xml"},
    {"name": "HuggingFace", "url": "https://huggingface.co/blog/feed.xml"},
    
    # --- å€Ÿé“æº (è§£å†³åçˆ¬/æ— RSSé—®é¢˜) ---
    {"name": "Anthropic(TC)", "url": "https://techcrunch.com/tag/anthropic/feed/"},
    {"name": "Meta AI(Eng)", "url": "https://engineering.fb.com/category/ai/feed/"},
    
    # --- ç¤¾åŒºä¸äº§å“ ---
    {"name": "ProductHunt", "url": "https://www.producthunt.com/feed?category=artificial-intelligence"},
    {"name": "GitHub Py", "url": "https://mshibanami.github.io/GitHubTrendingRSS/daily/python.xml"},
    {"name": "TheVerge", "url": "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml"},
    
    # --- å›½å†…æº ---
    {"name": "QbitAI", "url": "https://www.qbitai.com/feed"},
    {"name": "PyTorch", "url": "https://pytorch.org/blog/feed.xml"},
]

# === ğŸ› ï¸ å·¥å…·å‡½æ•° ===

def clean_html(raw_html):
    """å»é™¤æè¿°ä¸­çš„ HTML æ ‡ç­¾ï¼Œåªç•™çº¯æ–‡æœ¬"""
    if not raw_html: return ""
    cleanr = re.compile('<.*?>')
    text = re.sub(cleanr, '', raw_html)
    return text.replace('\n', ' ').strip()[:300] # æˆªæ–­ä¸€ä¸‹ï¼ŒçœToken

def parse_date(date_str):
    """ä¸‡èƒ½æ—¥æœŸæ¸…æ´—ï¼šç»Ÿä¸€è¿”å› YYYY-MM-DD HH:MM"""
    if not date_str: return "N/A"
    dt = None
    try:
        # 1. å°è¯• RFC 822 (RSS)
        parsed = email.utils.parsedate_to_datetime(date_str)
        if parsed: dt = parsed
    except:
        pass
        
    if not dt:
        try:
            # 2. å°è¯• ISO 8601 (Atom)
            clean_iso = date_str.replace('Z', '+00:00')
            dt = datetime.fromisoformat(clean_iso)
        except:
            pass
            
    if dt:
        return dt.strftime('%Y-%m-%d %H:%M')
    return date_str  # è§£æå¤±è´¥è¿”å›åŸæ ·

def extract_metadata(xml_text, source_name):
    """è§£æå•æ¡ XMLï¼Œè¿”å›ç»“æ„åŒ–æ•°æ® list å’Œ æœ€æ–°æ—¶é—´"""
    try:
        root = ET.fromstring(xml_text)
        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry') or root.findall('.//entry')
        
        parsed_items = []
        dates = []
        
        for item in items[:MAX_ITEMS]:
            # 1. æå– Title
            title_node = item.find('title')
            if title_node is None: title_node = item.find('{http://www.w3.org/2005/Atom}title')
            title = title_node.text.strip() if title_node is not None and title_node.text else "No Title"

            # 2. æå– Link
            link = "N/A"
            link_node = item.find('link')
            if link_node is not None:
                link = link_node.text or link_node.attrib.get('href')
            else:
                link_node = item.find('{http://www.w3.org/2005/Atom}link')
                if link_node is not None:
                    link = link_node.attrib.get('href')

            # 3. æå– Date
            raw_date = None
            for tag in ['pubDate', 'published', 'updated', 'dc:date']:
                node = item.find(tag) or item.find(f"{{http://www.w3.org/2005/Atom}}{tag}")
                if node is not None and node.text:
                    raw_date = node.text
                    break
            
            clean_date = parse_date(raw_date)
            if clean_date != "N/A": dates.append(clean_date)

            # 4. æå– Description
            desc = ""
            for tag in ['description', 'summary', 'content']:
                node = item.find(tag) or item.find(f"{{http://www.w3.org/2005/Atom}}{tag}")
                if node is not None and node.text:
                    desc = node.text
                    break
            clean_desc = clean_html(desc)

            # 5. å­˜å…¥ç»“æœ
            parsed_items.append({
                "source": source_name,
                "title": title,
                "url": link,
                "date": clean_date,
                "desc": clean_desc
            })
            
        latest = max(dates) if dates else "N/A"
        return parsed_items, latest
        
    except Exception as e:
        print(f"è§£æé”™è¯¯: {e}")
        return [], "Error"

# === ğŸš€ ä¸»é€»è¾‘ ===
def run_etl_pipeline():
    all_news_json = []  # å­˜æ”¾æ‰€æœ‰æ¸…æ´—å¥½çš„æ–°é—»
    report_lines = []   # å­˜æ”¾ç»™ç®¡ç†å‘˜çœ‹çš„æŠ¥å‘Š

    # æ§åˆ¶å°æ‰“å°è¡¨å¤´
    print(f"{'Source Name':<20} | {'Status':<5} | {'Count':<5} | {'Latest Date'}")
    print("-" * 80)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    }

    for src in RSS_URLS:
        name = src['name']
        url = src['url']
        
        status = "ğŸ”´"
        count = 0
        latest_date = "---"
        
        try:
            resp = requests.get(url, headers=headers, timeout=30)
            if resp.status_code == 200:
                # è§£ææ•°æ®
                items, latest_date = extract_metadata(resp.text, name)
                count = len(items)
                
                if count > 0:
                    status = "âœ…"
                    all_news_json.extend(items) # åŠ å…¥æ€»åˆ—è¡¨
                else:
                    status = "âš ï¸"
            else:
                status = "âŒ"
                latest_date = f"HTTP {resp.status_code}"
                
        except Exception as e:
            status = "âŒ"
            latest_date = "Conn Err"

        # æ‰“å°å¹¶è®°å½•æ—¥å¿—
        print(f"{name:<20} | {status} | {count:<5} | {latest_date}")
        report_lines.append(f"{status} [{name}] Items:{count} | Latest:{latest_date}")
        time.sleep(1)

    return all_news_json, "\n".join(report_lines)

def upload_to_cos(filename, content):
    if not content: return
    try:
        config = CosConfig(Region=os.environ['COS_REGION'], SecretId=os.environ['TENCENT_SECRET_ID'], SecretKey=os.environ['TENCENT_SECRET_KEY'])
        client = CosS3Client(config)
        client.put_object(
            Bucket=os.environ['COS_BUCKET'], Body=content.encode('utf-8'), Key=filename,
            StorageClass='STANDARD', ContentType='application/json; charset=utf-8' # æ³¨æ„è¿™é‡Œæ˜¯ json
        )
        print(f"ğŸ‰ Uploaded: {filename}")
    except Exception as e:
        print(f"âŒ Upload Failed {filename}: {e}")

if __name__ == "__main__":
    # 1. æŠ“å–ä¸æ¸…æ´—
    clean_data, report_txt = run_etl_pipeline()
    
    # 2. ä¸Šä¼  JSON æ•°æ® (ç»™ Coze æœºå™¨è¯»)
    # json.dumps å¤„ç†é ASCII å­—ç¬¦ï¼Œä¿æŒä¸­æ–‡å¯è¯»
    if clean_data:
        json_str = json.dumps(clean_data, ensure_ascii=False, indent=2)
        upload_to_cos('RSS/news.json', json_str)
    
    # 3. ä¸Šä¼  æŠ¥å‘Šæ–‡ä»¶ (ç»™ ç®¡ç†å‘˜ è¯»)
    if report_txt:
        upload_to_cos('RSS/rss_report.txt', report_txt)
