import os
import requests
import time
import xml.etree.ElementTree as ET
from datetime import datetime
from qcloud_cos import CosConfig
from qcloud_cos import CosS3Client

# === âœ… 2025å¹´12æœˆ ç»ˆæä¿®æ­£ç‰ˆ URL åˆ—è¡¨ ===
RSS_URLS = [
    # --- 1. æå…¶ç¨³å®šçš„å®˜æ–¹æº ---
    "https://openai.com/news/rss.xml",                # OpenAI (ç¨³)
    "https://deepmind.google/blog/rss.xml",           # DeepMind (ç¨³)
    "https://huggingface.co/blog/feed.xml",           # HuggingFace (ç¨³)
    "https://www.producthunt.com/feed?category=artificial-intelligence", # Product Hunt (ç¨³)
    "https://mshibanami.github.io/GitHubTrendingRSS/daily/python.xml",   # GitHubçƒ­æ¦œ (ç¨³)
    
    # --- 2. ä¿®æ­£åçš„æº ---
    
    # PyTorch: ä¿®æ­£ URL è·¯å¾„
    "https://pytorch.org/blog/feed.xml",
    
    # Meta AI: å®˜ç½‘åçˆ¬å¤ªä¸¥(400)ï¼Œæ”¹ç”¨ Meta å·¥ç¨‹åšå®¢ AI åˆ†ç±» (WordPressæ¶æ„ï¼Œéå¸¸ç¨³)
    "https://engineering.fb.com/category/ai/feed/",
    
    # Stability AI: ä¿®æ­£å‚æ•°
    "https://stability.ai/news?format=rss",

    # --- 3. "å€Ÿåˆ€æ€äºº"æº (ä¸“é—¨è§£å†³æ— RSS/åœæ›´é—®é¢˜) ---
    
    # Anthropic: å®˜ç½‘æ— RSSï¼Œç¤¾åŒºæºåœæ›´ã€‚æ”¹ç”¨ TechCrunch çš„ Anthropic ä¸“å±æ ‡ç­¾
    # åªè¦ Anthropic å‘æ–°é—»ï¼ŒTechCrunch è‚¯å®šç¬¬ä¸€æ—¶é—´æŠ¥ã€‚
    "https://techcrunch.com/tag/anthropic/feed/",
    
    # è¡¥å……: The Verge AI (æ›¿ä»£åçˆ¬ä¸¥é‡çš„ Ben Evans)
    "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml",
    
    # --- 4. å›½å†…æº (ä½œä¸ºæ•°æ®ä¿åº•) ---
    "https://www.qbitai.com/feed"
]

# === XML è§£æä¸æ¸…æ´—å·¥å…· ===
def analyze_xml(xml_text):
    try:
        # é¢„å¤„ç†ï¼šæœ‰äº›æº xml å£°æ˜ç¼–ç å¯èƒ½æœ‰è¯¯ï¼Œå¼ºè¡Œå¿½ç•¥é”™è¯¯è§£ç 
        root = ET.fromstring(xml_text)
        items = root.findall('.//item')
        if not items: items = root.findall('.//{http://www.w3.org/2005/Atom}entry')
        if not items: items = root.findall('.//entry')
            
        count = len(items)
        latest_date = "N/A"
        
        # æ‰¾æœ€æ–°æ—¥æœŸ (å‰3æ¡)
        for item in items[:3]:
            # ä¼˜å…ˆæ‰¾ pubDate (RSS)
            node = item.find('pubDate')
            # å…¶æ¬¡æ‰¾ published (Atom)
            if node is None: node = item.find('{http://www.w3.org/2005/Atom}published')
            # å†æ¬¡æ‰¾ updated
            if node is None: node = item.find('{http://www.w3.org/2005/Atom}updated')
            # å†æ¬¡æ‰¾ dc:date
            if node is None: node = item.find('{http://purl.org/dc/elements/1.1/}date')
            
            if node is not None and node.text:
                # æˆªæ–­æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¤ªé•¿äº†æ²¡æ³•çœ‹
                latest_date = node.text[:25]
                break
                
        return count, latest_date
    except Exception:
        return 0, "Parse Error"

def fetch_and_report():
    combined_data = ""
    report_lines = []
    
    print(f"{'RSS æº (Short URL)':<40} | {'St':<2} | {'Num':<3} | {'Latest Date'}")
    print("-" * 85)
    
    report_lines.append(f"æ›´æ–°æ—¶é—´: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC")
    report_lines.append("-" * 60)
    
    # ä¼ªè£…å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Referer': 'https://google.com'
    }

    for url in RSS_URLS:
        status_icon = "ğŸ”´"
        count = 0
        latest = "---"
        # ç®€åŒ– URL æ˜¾ç¤º
        short_url = url.replace("https://", "").replace("www.", "").replace("techcrunch.com/tag/", "TC/").replace("feed/", "")[:38]

        try:
            resp = requests.get(url, headers=headers, timeout=25)
            
            # å…¼å®¹ï¼šæœ‰äº›æœåŠ¡å™¨è¿”å› 403 ä½†å…¶å®ç»™äº†å†…å®¹ï¼ˆç½•è§ï¼‰ï¼Œä¸»è¦çœ‹ 200
            if resp.status_code == 200 and len(resp.text) > 500:
                status_icon = "âœ…"
                count, latest = analyze_xml(resp.text)
                
                # åªæœ‰è§£æå‡ºæ¡ç›®çš„æ‰ç®—çœŸæ­£æˆåŠŸ
                if count > 0:
                    combined_data += f"\n\n<<<<SOURCE_START:{url}>>>>\n"
                    combined_data += resp.text
                    combined_data += f"\n<<<<SOURCE_END>>>>\n"
                else:
                    status_icon = "âš ï¸"
                    latest = "Xml Empty"
            else:
                status_icon = "âŒ"
                latest = f"HTTP {resp.status_code}"

        except Exception as e:
            status_icon = "âŒ"
            latest = "Err"

        print(f"{short_url:<40} | {status_icon} | {count:<3} | {latest}")
        
        report_lines.append(f"{status_icon} {short_url}")
        report_lines.append(f"   Items: {count} | Last: {latest}")
        
        time.sleep(2)

    return combined_data, "\n".join(report_lines)

def upload_to_cos(filename, content):
    if not content: return
    secret_id = os.environ['TENCENT_SECRET_ID']
    secret_key = os.environ['TENCENT_SECRET_KEY']
    region = os.environ['COS_REGION']
    bucket = os.environ['COS_BUCKET']
    
    config = CosConfig(Region=region, SecretId=secret_id, SecretKey=secret_key)
    client = CosS3Client(config)
    
    try:
        client.put_object(
            Bucket=bucket, Body=content.encode('utf-8'), Key=filename,
            StorageClass='STANDARD', ContentType='text/plain; charset=utf-8'
        )
        print(f"ğŸ‰ Upload Success: {filename}")
    except Exception as e:
        print(f"âŒ Upload Failed {filename}: {e}")

if __name__ == "__main__":
    full_data, report_text = fetch_and_report()
    if len(full_data) > 500:
        upload_to_cos('RSS/rss_mirror.txt', full_data)
        upload_to_cos('RSS/rss_report.txt', report_text)
    else:
        print("âš ï¸ æ•°æ®é‡ä¸¥é‡ä¸è¶³ï¼Œè·³è¿‡ä¸Šä¼ ã€‚")
