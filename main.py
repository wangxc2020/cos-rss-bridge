import os
import requests
import time
import json
import re
import feedparser
from datetime import datetime, timezone, timedelta
import email.utils
from qcloud_cos import CosConfig
from qcloud_cos import CosS3Client

# === âš™ï¸ é…ç½®åŒºåŸŸ ===
MAX_ITEMS = 5  # æ¯ä¸ªæºæœ€å¤šæŠ“å–æœ€æ–°çš„ N æ¡

# === ğŸ”— ä¼˜è´¨æºåˆ—è¡¨ ===
RSS_URLS = [
    # --- æ ¸å¿ƒå·¨å¤´ ---
    {"name": "OpenAI", "url": "https://openai.com/news/rss.xml"},
    {"name": "DeepMind", "url": "https://deepmind.google/blog/rss.xml"},
    {"name": "HuggingFace", "url": "https://huggingface.co/blog/feed.xml"},
    
    # --- å€Ÿé“æº (è§£å†³åçˆ¬/æ— RSSé—®é¢˜) ---
    {"name": "Anthropic(TC)", "url": "https://techcrunch.com/tag/anthropic/feed/"},
    {"name": "Meta AI(Eng)", "url": "https://engineering.fb.com/category/ai/feed/"},
    
    # --- ç¤¾åŒºä¸äº§å“ ---
    {"name": "ProductHunt", "url": "https://www.producthunt.com/feed?category=artificial-intelligence"},
    {"name": "GitHub Py", "url": "https://mshibanami.github.io/GitHubTrendingRSS/daily/python.xml"},
    {"name": "TheVerge", "url": "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml"},
    
    # --- å›½å†…æº ---
    {"name": "QbitAI", "url": "https://www.qbitai.com/feed"},
    {"name": "PyTorch", "url": "https://pytorch.org/blog/feed.xml"},
]

# === ğŸ› ï¸ å·¥å…·å‡½æ•° ===

def clean_html(raw_html):
    """å»é™¤æè¿°ä¸­çš„ HTML æ ‡ç­¾ï¼Œåªç•™çº¯æ–‡æœ¬"""
    if not raw_html: return ""
    cleanr = re.compile('<.*?>')
    text = re.sub(cleanr, '', raw_html)
    return text.replace('\n', ' ').strip()[:300] # æˆªæ–­ä¸€ä¸‹ï¼ŒçœToken

def parse_date(date_obj_or_str):
    """ä¸‡èƒ½æ—¥æœŸæ¸…æ´—ï¼šç»Ÿä¸€è¿”å› YYYY-MM-DD HH:MM"""
    if not date_obj_or_str: return "N/A"
    
    # å¦‚æœå·²ç»æ˜¯ struct_time (feedparser è§£æç»“æœ)
    if isinstance(date_obj_or_str, time.struct_time):
        return time.strftime('%Y-%m-%d %H:%M', date_obj_or_str)

    date_str = str(date_obj_or_str)
    dt = None
    try:
        # 1. å°è¯• RFC 822 (RSS)
        parsed = email.utils.parsedate_to_datetime(date_str)
        if parsed: dt = parsed
    except:
        pass
        
    if not dt:
        try:
            # 2. å°è¯• ISO 8601 (Atom)
            clean_iso = date_str.replace('Z', '+00:00')
            dt = datetime.fromisoformat(clean_iso)
        except:
            pass
            
    if dt:
        return dt.strftime('%Y-%m-%d %H:%M')
    return date_str  # è§£æå¤±è´¥è¿”å›åŸæ ·

def extract_metadata(xml_content, source_name):
    """è§£æ RSS/Atom å†…å®¹ (ä½¿ç”¨ feedparser)"""
    try:
        # feedparser æœ€å¥½ç›´æ¥å¤„ç† bytesï¼Œä»¥ä¾¿å®ƒè‡ªå·±å¤„ç†ç¼–ç 
        feed = feedparser.parse(xml_content)
        
        parsed_items = []
        dates = []
        
        for item in feed.entries[:MAX_ITEMS]:
            # 1. æå– Title
            title = item.get('title', 'No Title')

            # 2. æå– Link
            link = item.get('link', 'N/A')

            # 3. æå– Date
            # feedparser é€šå¸¸ä¼šæä¾› parsed åçš„ struct_time
            raw_date = item.get('published_parsed') or item.get('updated_parsed')
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•è·å–åŸå§‹å­—ç¬¦ä¸²
            if not raw_date:
                raw_date = item.get('published') or item.get('updated') or item.get('date')

            clean_date = parse_date(raw_date)
            if clean_date != "N/A": dates.append(clean_date)

            # 4. æå– Description
            # ä¼˜å…ˆæ‰¾ summary / description
            desc = item.get('summary') or item.get('description') or ""
            
            # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•æ‰¾ content (é€šå¸¸æ˜¯ list)
            if not desc and 'content' in item:
                # content æ˜¯ä¸€ä¸ª listï¼Œé‡Œé¢å¯èƒ½æœ‰ html æˆ– text
                for c in item.content:
                    if c.get('value'):
                        desc = c.get('value')
                        break
            
            clean_desc = clean_html(desc)

            # 5. å­˜å…¥ç»“æœ
            parsed_items.append({
                "source": source_name,
                "title": title,
                "url": link,
                "date": clean_date,
                "desc": clean_desc
            })
            
        latest = max(dates) if dates else "N/A"
        return parsed_items, latest
        
    except Exception as e:
        print(f"è§£æé”™è¯¯: {e}")
        return [], "Error"

# === ğŸš€ ä¸»é€»è¾‘ ===
def run_etl_pipeline():
    all_news_json = []  # å­˜æ”¾æ‰€æœ‰æ¸…æ´—å¥½çš„æ–°é—»
    report_lines = []   # å­˜æ”¾ç»™ç®¡ç†å‘˜çœ‹çš„æŠ¥å‘Š

    # æ§åˆ¶å°æ‰“å°è¡¨å¤´
    print(f"{'Source Name':<20} | {'Status':<5} | {'Count':<5} | {'Latest Date'}")
    print("-" * 80)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    }

    for src in RSS_URLS:
        name = src['name']
        url = src['url']
        
        status = "ğŸ”´"
        count = 0
        latest_date = "---"
        
        try:
            resp = requests.get(url, headers=headers, timeout=30)
            if resp.status_code == 200:
                # è§£ææ•°æ®
                # ä¼ å…¥ resp.content (bytes) ç»™ feedparser
                items, latest_date = extract_metadata(resp.content, name)
                count = len(items)
                
                if count > 0:
                    status = "âœ…"
                    all_news_json.extend(items) # åŠ å…¥æ€»åˆ—è¡¨
                else:
                    status = "âš ï¸"
            else:
                status = "âŒ"
                latest_date = f"HTTP {resp.status_code}"
                
        except Exception as e:
            status = "âŒ"
            latest_date = "Conn Err"

        # æ‰“å°å¹¶è®°å½•æ—¥å¿—
        print(f"{name:<20} | {status} | {count:<5} | {latest_date}")
        report_lines.append(f"{status} [{name}] Items:{count} | Latest:{latest_date}")
        time.sleep(1)

    return all_news_json, "\n".join(report_lines)

def upload_to_cos(filename, content):
    if not content: return
    try:
        config = CosConfig(Region=os.environ['COS_REGION'], SecretId=os.environ['TENCENT_SECRET_ID'], SecretKey=os.environ['TENCENT_SECRET_KEY'])
        client = CosS3Client(config)
        client.put_object(
            Bucket=os.environ['COS_BUCKET'], Body=content.encode('utf-8'), Key=filename,
            StorageClass='STANDARD', ContentType='application/json; charset=utf-8' # æ³¨æ„è¿™é‡Œæ˜¯ json
        )
        print(f"ğŸ‰ Uploaded: {filename}")
    except Exception as e:
        print(f"âŒ Upload Failed {filename}: {e}")

if __name__ == "__main__":
    # 1. æŠ“å–ä¸æ¸…æ´—
    clean_data, report_txt = run_etl_pipeline()
    
    # 2. ä¸Šä¼  JSON æ•°æ® (ç»™ Coze æœºå™¨è¯»)
    # json.dumps å¤„ç†é ASCII å­—ç¬¦ï¼Œä¿æŒä¸­æ–‡å¯è¯»
    if clean_data:
        json_str = json.dumps(clean_data, ensure_ascii=False, indent=2)
        upload_to_cos('RSS/news.json', json_str)
    
    # 3. ä¸Šä¼  æŠ¥å‘Šæ–‡ä»¶ (ç»™ ç®¡ç†å‘˜ è¯»)
    if report_txt:
        upload_to_cos('RSS/rss_report.txt', report_txt)
