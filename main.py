import os
import requests
import time
import xml.etree.ElementTree as ET
from datetime import datetime
from qcloud_cos import CosConfig
from qcloud_cos import CosS3Client

# === âœ… ç»è¿‡äººå·¥æ¸…æ´—ã€ä¿®å¤åçš„å®Œæ•´æºåˆ—è¡¨ ===
RSS_URLS = [
    # --- 1. å…¨çƒ AI å·¨å¤´ (Core) ---
    "https://openai.com/news/rss.xml",                # OpenAI å®˜æ–¹
    "https://deepmind.google/blog/rss.xml",           # Google DeepMind å®˜æ–¹
    "https://ai.meta.com/blog/rss.xml",               # Meta AI (Facebook) å®˜æ–¹
    # Anthropic å®˜ç½‘æ—  RSSï¼Œä½¿ç”¨ GitHub ç¤¾åŒºæ¯å¤©æ›´æ–°çš„é™æ€é•œåƒ
    "https://raw.githubusercontent.com/Olshansk/rss-feeds/main/feeds/feed_anthropic_news.xml",
    
    # --- 2. å¼€å‘è€…ä¸å¼€æºç¤¾åŒº (Dev) ---
    "https://huggingface.co/blog/feed.xml",           # HuggingFace åšå®¢
    "https://pytorch.org/feed.xml",                   # PyTorch æ¡†æ¶åŠ¨æ€
    # GitHub Python çƒ­æ¦œ (é™æ€é•œåƒï¼Œæç¨³)
    "https://mshibanami.github.io/GitHubTrendingRSS/daily/python.xml",
    
    # --- 3. è¡Œä¸šåˆ†æä¸è¶‹åŠ¿ (Insights) ---
    "https://lastweekin.ai/feed",                     # Last Week in AI (é«˜è´¨é‡æ±‡æ€»)
    "https://www.ben-evans.com/benedictevans?format=xml", # Benedict Evans (æ·±åº¦åˆ†æï¼Œå·²ä¿®æ­£é“¾æ¥)
    
    # --- 4. æ–°äº§å“å‘ç° (Product) ---
    "https://www.producthunt.com/feed?category=artificial-intelligence", # Product Hunt AIæ¦œ
    
    # --- 5. å›½å†…åª’ä½“ (åœ¨ GitHub æŠ“å–æ˜¯é˜²æ­¢å›½å†…æœåŠ¡å™¨æ³¢åŠ¨ï¼Œä½œä¸ºå¤‡ä»½) ---
    "https://www.qbitai.com/feed"                     # é‡å­ä½ (å·²ä¿®æ­£ feet -> feed)
]

# === ç®€å•çš„ XML å…ƒæ•°æ®åˆ†æå·¥å…· ===
def analyze_xml(xml_text):
    try:
        root = ET.fromstring(xml_text)
        items = root.findall('.//item')
        if not items: items = root.findall('.//{http://www.w3.org/2005/Atom}entry')
        if not items: items = root.findall('.//entry')
            
        count = len(items)
        latest_date = "N/A"
        
        # æ‰¾æœ€æ–°æ—¥æœŸ
        for item in items[:3]:
            for tag in ['pubDate', 'published', 'updated', 'dc:date']:
                node = item.find(tag)
                if node is None: node = item.find(f"{{http://www.w3.org/2005/Atom}}{tag}")
                if node is not None and node.text:
                    latest_date = node.text[:25]
                    break
            if latest_date != "N/A": break
        return count, latest_date
    except:
        return 0, "Parse Error"

def fetch_and_report():
    combined_data = ""
    report_lines = []
    
    # æ‰“å°æ§åˆ¶å°è¡¨å¤´
    print(f"{'RSS æº (Short URL)':<40} | {'St':<2} | {'Num':<3} | {'Latest Date'}")
    print("-" * 85)
    
    report_lines.append(f"æ›´æ–°æ—¶é—´: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC")
    report_lines.append("-" * 60)
    
    # å¼ºåŠ›æµè§ˆå™¨ä¼ªè£…å¤´ (è§£å†³ Product Hunt / Ben Evans åçˆ¬)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Referer': 'https://google.com'
    }

    for url in RSS_URLS:
        status_icon = "ğŸ”´"
        count = 0
        latest = "---"
        short_url = url.replace("https://", "").replace("www.", "").replace("raw.githubusercontent.com", "github_raw")[:38]

        try:
            # 30ç§’è¶…æ—¶ï¼Œé˜²æ­¢å¤§æ–‡ä»¶å¡æ­»
            resp = requests.get(url, headers=headers, timeout=30)
            
            if resp.status_code == 200 and len(resp.text) > 100:
                status_icon = "âœ…"
                count, latest = analyze_xml(resp.text)
                
                # æ‹¼æ¥æ•°æ® (æ‰“ä¸Šæ ‡ç­¾ï¼Œæ–¹ä¾¿ Coze è¯†åˆ«æ¥æº)
                combined_data += f"\n\n<<<<SOURCE_START:{url}>>>>\n"
                combined_data += resp.text
                combined_data += f"\n<<<<SOURCE_END>>>>\n"
            else:
                status_icon = "âš ï¸"
                latest = f"HTTP {resp.status_code}"

        except Exception as e:
            status_icon = "âŒ"
            latest = "Err" # ç®€åŒ–æŠ¥é”™æ˜¾ç¤º

        # æ‰“å°è¿›åº¦
        print(f"{short_url:<40} | {status_icon} | {count:<3} | {latest}")
        
        # å†™å…¥æŠ¥å‘Š
        report_lines.append(f"{status_icon} {short_url}")
        report_lines.append(f"   Items: {count} | Latest: {latest}")
        
        # ä¼‘æ¯ 2 ç§’ï¼Œé˜²å°
        time.sleep(2)

    return combined_data, "\n".join(report_lines)

def upload_to_cos(filename, content):
    if not content: return
    
    # ä» GitHub Secrets è·å–å¯†é’¥
    secret_id = os.environ['TENCENT_SECRET_ID']
    secret_key = os.environ['TENCENT_SECRET_KEY']
    region = os.environ['COS_REGION']
    bucket = os.environ['COS_BUCKET']
    
    config = CosConfig(Region=region, SecretId=secret_id, SecretKey=secret_key)
    client = CosS3Client(config)
    
    try:
        client.put_object(
            Bucket=bucket,
            Body=content.encode('utf-8'),
            Key=filename,
            StorageClass='STANDARD',
            ContentType='text/plain; charset=utf-8'
        )
        print(f"ğŸ‰ Upload Success: {filename}")
    except Exception as e:
        print(f"âŒ Upload Failed {filename}: {e}")

if __name__ == "__main__":
    # 1. æ‰§è¡ŒæŠ“å–
    full_data, report_text = fetch_and_report()
    
    # 2. ä¸Šä¼ æ•°æ® (åªè¦æœ‰æ•°æ®å°±ä¼ )
    if len(full_data) > 500:
        upload_to_cos('rss_mirror.txt', full_data)
        upload_to_cos('rss_report.txt', report_text)
    else:
        print("âš ï¸ æ•°æ®é‡è¿‡å°‘ (<500b)ï¼Œæ”¾å¼ƒä¸Šä¼ ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æºã€‚")
